# ═══════════════════════════════════════════════════════════════════════════════
# ARIA — Configuration LLM
# ═══════════════════════════════════════════════════════════════════════════════

# ── LLM Local (défaut) ───────────────────────────────────────────────────────
# Par défaut, ARIA utilise Ollama + Qwen3 8B en local.
# Aucune configuration nécessaire si Ollama tourne sur votre machine.
#
# Installation :
#   1. Installer Ollama : https://ollama.com/download
#   2. Télécharger le modèle : ollama pull qwen3:8b
#   3. Lancer Ollama : ollama serve  (ou il tourne déjà en tâche de fond)
#   4. Lancer ARIA : npm run dev
#
# Optionnel : personnaliser l'URL ou le modèle Ollama
# VITE_OLLAMA_BASE_URL=http://localhost:11434
# VITE_OLLAMA_MODEL=qwen3:8b

# ── LLM Externe (optionnel) ─────────────────────────────────────────────────
# Si vous configurez une clé API externe, elle sera utilisée en priorité.
# En cas d'échec de l'API externe, ARIA bascule automatiquement sur Ollama local.
#
# Le provider est auto-détecté depuis le format de la clé :
#   gsk_...    → Groq        (https://console.groq.com)
#   AIzaSy...  → Gemini      (https://aistudio.google.com)
#   sk-...     → OpenAI      (https://platform.openai.com)
#   sk-ant-... → Anthropic   (https://console.anthropic.com)
#   sk-or-...  → OpenRouter  (https://openrouter.ai)
# VITE_LLM_API_KEY=your_api_key_here

# ── Endpoint custom (optionnel) ─────────────────────────────────────────────
# Pour utiliser un endpoint OpenAI-compatible (Mistral, Azure, etc.)
# Si défini, remplace l'URL auto-détectée du provider.
# VITE_LLM_BASE_URL=https://api.mistral.ai/v1
