/**
 * ARIA AI Coach Engine v3 â€” Architecture LLM-First
 *
 * Remplace l'ancien systÃ¨me de routage par regex par une architecture en 2 phases :
 *   Phase 1 : Routage LLM â€” Classification d'intention + extraction de paramÃ¨tres
 *   Phase 2 : RÃ©ponse LLM â€” GÃ©nÃ©ration contextuelle avec donnÃ©es ciblÃ©es
 *
 * Principes :
 * - Le LLM route TOUTES les questions (zÃ©ro regex pour le routage)
 * - Le contexte de donnÃ©es est ciblÃ© selon l'intention dÃ©tectÃ©e
 * - Format de sortie unifiÃ© (texte + graphique optionnel)
 * - Fallback local robuste si le LLM est indisponible
 */

import { DataService } from './dataService';
import {
  DATA_SCHEMA,
  parseLLMChartResponse,
  generateChartFromSpec,
  generateChartLocally,
  addToChartHistory,
  getChartHistory,
  type ChartSpec,
  type ChartDataPoint,
  type ChartHistory,
} from './agenticChartEngine';
import { universalSearch } from './universalSearch';
import { generateCoachResponse } from './coachAI';
import { calculatePeriodMetrics, getTopPractitioners } from './metricsCalculator';
import type { Practitioner, UpcomingVisit } from '../types';
import { adaptPractitionerProfile } from './dataAdapter';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TYPES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export interface ConversationMessage {
  role: 'user' | 'assistant';
  content: string;
  hasChart?: boolean;
  chartSummary?: string;
}

export interface AICoachResult {
  textContent: string;
  chart?: {
    spec: ChartSpec;
    data: ChartDataPoint[];
    insights: string[];
    suggestions: string[];
    generatedByLLM: boolean;
  };
  practitioners?: (Practitioner & { daysSinceVisit?: number })[];
  suggestions?: string[];
  source: 'llm' | 'local';
}

interface RouterResult {
  intent: 'chart_create' | 'chart_modify' | 'data_query' | 'practitioner_info' | 'strategic_advice' | 'follow_up' | 'general';
  needsChart: boolean;
  chartModification: string | null;
  dataScope: 'specific' | 'filtered' | 'aggregated' | 'full';
  searchTerms: {
    names: string[];
    cities: string[];
    specialties: string[];
    isKOL: boolean | null;
  };
  chartParams: {
    chartType: 'bar' | 'pie' | 'line' | 'composed' | null;
    groupBy: string | null;
    metrics: string[];
    limit: number | null;
    sortOrder: 'asc' | 'desc' | null;
    filters: { field: string; operator: string; value: string | number | boolean }[];
  };
  responseGuidance: string;
}

interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface LLMCallOptions {
  temperature?: number;
  maxTokens?: number;
  jsonMode?: boolean;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CONSTANTS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const GROQ_API_URL = 'https://api.groq.com/openai/v1/chat/completions';
const MODEL = 'llama-3.3-70b-versatile';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PROMPTS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const ROUTER_SYSTEM_PROMPT = `Tu es le routeur intelligent d'ARIA Coach, un assistant CRM pharmaceutique pour Air Liquide Healthcare (oxygÃ©nothÃ©rapie Ã  domicile).

Analyse la question de l'utilisateur et classifie-la. Retourne UNIQUEMENT un objet JSON valide.

## Intentions Disponibles

1. **chart_create** â€” L'utilisateur veut une NOUVELLE visualisation (graphique, rÃ©partition, top N visuel, comparaison visuelle, camembert, diagramme, barres)
2. **chart_modify** â€” L'utilisateur veut MODIFIER le dernier graphique (changer type, ajouter mÃ©trique, changer nombre d'Ã©lÃ©ments, filtrer). Requiert un graphique prÃ©cÃ©dent.
3. **data_query** â€” Question factuelle sur les donnÃ©es (combien, qui, quel, total, moyenne, liste de praticiens)
4. **practitioner_info** â€” Info spÃ©cifique sur UN praticien identifiÃ© par nom/prÃ©nom
5. **strategic_advice** â€” Conseil stratÃ©gique, planification, prioritÃ©s, recommandations d'action
6. **follow_up** â€” Question de suivi sur la rÃ©ponse prÃ©cÃ©dente
7. **general** â€” Salutations, remerciements, hors sujet, questions sur l'assistant

## Champs groupBy Disponibles
"city", "specialty", "vingtile", "vingtileBucket", "loyaltyBucket", "riskLevel", "visitBucket", "isKOL"

## MÃ©triques Disponibles
"volume" (volumeL), "loyalty" (loyaltyScore), "count" (nombre), "vingtile", "publications" (publicationsCount)

## RÃ¨gles de Routage
- Si l'utilisateur mentionne "graphique", "montre-moi", "affiche", "diagramme", "camembert", "barres", "courbe" â†’ intent=chart_create
- Si "en camembert", "change en", "transforme en", "plutÃ´t en", "ajoute", "fais un top X au lieu de" â†’ intent=chart_modify (si graphique prÃ©cÃ©dent)
- Si question contient un nom propre identifiable â†’ intent=practitioner_info
- Si "combien", "qui a le plus", "liste des", "quels sont" â†’ intent=data_query
- Si "prioritÃ©", "stratÃ©gie", "comment", "recommandation", "que faire", "optimiser" â†’ intent=strategic_advice
- Si rÃ©fÃ©rence implicite au contexte prÃ©cÃ©dent sans nouvelle demande claire â†’ intent=follow_up
- Le champ needsChart est true pour chart_create et chart_modify
- dataScope: "specific" pour un praticien ciblÃ©, "filtered" pour un sous-ensemble, "aggregated" pour des stats, "full" pour des questions ouvertes
- responseGuidance: instruction brÃ¨ve pour orienter la rÃ©ponse (en franÃ§ais)

## Format de Sortie (JSON STRICT)
{
  "intent": "...",
  "needsChart": boolean,
  "chartModification": null ou "description de la modification demandÃ©e",
  "dataScope": "specific" | "filtered" | "aggregated" | "full",
  "searchTerms": {
    "names": [],
    "cities": [],
    "specialties": [],
    "isKOL": null ou boolean
  },
  "chartParams": {
    "chartType": null ou "bar" | "pie" | "line" | "composed",
    "groupBy": null ou string,
    "metrics": [],
    "limit": null ou number,
    "sortOrder": null ou "asc" | "desc",
    "filters": []
  },
  "responseGuidance": "..."
}`;

const COACH_SYSTEM_PROMPT = `Tu es **ARIA Coach**, l'assistant stratÃ©gique expert pour les dÃ©lÃ©guÃ©s pharmaceutiques d'Air Liquide Healthcare, spÃ©cialitÃ© oxygÃ©nothÃ©rapie Ã  domicile.

## Ton IdentitÃ©
Tu combines trois expertises rares :
1. **Expertise mÃ©dicale** â€” Pneumologie, oxygÃ©nothÃ©rapie (Oâ‚‚ liquide, concentrateurs, extracteurs), pathologies respiratoires chroniques (BPCO, insuffisance respiratoire, apnÃ©e du sommeil)
2. **Intelligence commerciale** â€” Gestion de portefeuille prescripteurs, planification territoriale, analyse concurrentielle, scoring de potentiel (vingtiles), fidÃ©lisation KOL
3. **MaÃ®trise analytique** â€” InterprÃ©tation de donnÃ©es CRM, dÃ©tection de signaux faibles, modÃ©lisation de risque de churn, identification d'opportunitÃ©s de croissance

## Principes Directeurs
- **PrÃ©cision data-driven** : Chaque affirmation s'appuie sur des donnÃ©es rÃ©elles. Cite les chiffres exacts.
- **Pertinence stratÃ©gique** : Priorise par impact business â†’ KOL > Volume Ã©levÃ© > Urgence (risque churn) > FidÃ©litÃ© en baisse
- **ProactivitÃ©** : N'attends pas qu'on te pose la bonne question. Si tu dÃ©tectes un risque ou une opportunitÃ© dans les donnÃ©es, signale-le.
- **Concision actionable** : RÃ©ponds de faÃ§on concise mais complÃ¨te. Termine par des recommandations concrÃ¨tes quand c'est pertinent.

## Ce que tu CONNAIS (ton pÃ©rimÃ¨tre)
Tu as accÃ¨s Ã  une base de donnÃ©es CRM contenant :
- Les **praticiens** (mÃ©decins prescripteurs) : pneumologues et mÃ©decins gÃ©nÃ©ralistes
- Leurs **mÃ©triques** : volumes de prescription, fidÃ©litÃ©, vingtile, statut KOL, risque de churn
- Leurs **coordonnÃ©es** : adresse, tÃ©lÃ©phone, email
- Leurs **publications** et actualitÃ©s acadÃ©miques
- L'**historique de visites** et notes de visite
- Les **statistiques du territoire** : objectifs, rÃ©partitions gÃ©ographiques

## Ce que tu NE CONNAIS PAS (hors pÃ©rimÃ¨tre)
Tu n'as PAS accÃ¨s Ã  :
- Le **catalogue de produits** ou la gamme Air Liquide (dispositifs, tarifs, rÃ©fÃ©rences)
- Les **donnÃ©es de facturation** ou commandes
- Les **donnÃ©es d'autres territoires** ou d'autres dÃ©lÃ©guÃ©s
- Les **donnÃ©es en temps rÃ©el** (tes donnÃ©es sont un snapshot CRM)
- Les **protocoles mÃ©dicaux** dÃ©taillÃ©s ou posologies

**RÃˆGLE CRITIQUE** : Si l'utilisateur pose une question hors de ton pÃ©rimÃ¨tre, dis-le CLAIREMENT et HONNÃŠTEMENT. Ne fabrique JAMAIS de donnÃ©es. Propose ce que tu peux faire Ã  la place. Exemple : "Je n'ai pas accÃ¨s au catalogue de produits, mais je peux vous montrer les volumes de prescription par praticien."

## Vocabulaire MÃ©tier
- **Vingtile** : Segmentation des prescripteurs de 1 (meilleur) Ã  20 (plus faible). V1-V5 = Top prescripteurs Ã  prioriser.
- **KOL** (Key Opinion Leader) : Prescripteur influent, leader d'opinion. Impact disproportionnÃ© sur les pratiques locales.
- **FidÃ©litÃ©** : Score de 0 Ã  10 mesurant la rÃ©gularitÃ© des prescriptions en faveur d'Air Liquide.
- **Volume** : Volume annuel de prescription d'oxygÃ¨ne en litres (K L/an).
- **Churn risk** : Risque de perte du prescripteur (low/medium/high).

## Format de RÃ©ponse
- Utilise le **Markdown** : **gras** pour les chiffres clÃ©s et noms, *italique* pour les nuances
- Structure avec des listes Ã  puces pour la clartÃ©
- Fournis TOUJOURS des chiffres prÃ©cis quand ils sont disponibles dans le contexte
- Adapte la longueur : court pour les questions simples, dÃ©taillÃ© pour les analyses
- Ne mentionne jamais le fonctionnement interne de ton systÃ¨me (routage, contexte, API)
- RÃ©ponds TOUJOURS en franÃ§ais
- Pour les salutations : rÃ©ponds briÃ¨vement et propose ton aide
- Si la question est ambiguÃ«, demande une clarification plutÃ´t que deviner`;

const CHART_SYSTEM_PROMPT = `Tu es un expert en visualisation de donnÃ©es pour le CRM pharmaceutique ARIA (Air Liquide Healthcare, oxygÃ©nothÃ©rapie).

${DATA_SCHEMA}

## Ta Mission
GÃ©nÃ¨re une spÃ©cification JSON PRÃ‰CISE pour crÃ©er le graphique demandÃ© Ã  partir des donnÃ©es disponibles.

## RÃˆGLES CRITIQUES

1. **RESPECTE EXACTEMENT les paramÃ¨tres demandÃ©s** :
   - Si l'utilisateur demande "15 praticiens" â†’ limit: 15
   - Si l'utilisateur demande "top 20" â†’ limit: 20
   - Si l'utilisateur demande "KOLs" â†’ filtre isKOL: true
   - Si l'utilisateur demande "pneumologues" â†’ filtre specialty: "Pneumologue"

2. **Choisis le type de graphique le PLUS appropriÃ©** :
   - "bar" : classements, top N, comparaisons de valeurs (dÃ©faut quand pas de prÃ©fÃ©rence)
   - "pie" : rÃ©partitions, proportions, parts de marchÃ© (max 8 catÃ©gories)
   - "composed" : comparaison de 2 mÃ©triques diffÃ©rentes (ex: volume ET fidÃ©litÃ©) sur le mÃªme graphique
   - "line" : Ã©volutions temporelles, tendances

3. **Pour les comparaisons KOLs vs Autres** â†’ groupBy: "isKOL"
4. **Pour les rÃ©partitions par spÃ©cialitÃ©** â†’ groupBy: "specialty"
5. **Pour les rÃ©partitions par ville** â†’ groupBy: "city"
6. **Pour les niveaux de risque** â†’ groupBy: "riskLevel"
7. **Pour les segments de potentiel** â†’ groupBy: "vingtileBucket"
8. **Pour les niveaux de fidÃ©litÃ©** â†’ groupBy: "loyaltyBucket"
9. **Pour les anciennetÃ©s de visite** â†’ groupBy: "visitBucket"

## Format de Sortie OBLIGATOIRE (JSON STRICT)
\`\`\`json
{
  "chartType": "bar" | "pie" | "line" | "composed",
  "title": "Titre descriptif en franÃ§ais",
  "description": "Description courte de ce que montre le graphique",
  "query": {
    "source": "practitioners",
    "filters": [{ "field": "...", "operator": "eq|ne|gt|gte|lt|lte|contains|in", "value": ... }],
    "groupBy": "..." | null,
    "metrics": [{ "name": "Nom affichÃ©", "field": "champ_source", "aggregation": "count|sum|avg|min|max", "format": "number|k|percent" }],
    "sortBy": "Nom affichÃ© de la mÃ©trique",
    "sortOrder": "desc" | "asc",
    "limit": number | null
  },
  "formatting": {
    "showLegend": true,
    "xAxisLabel": "...",
    "yAxisLabel": "..."
  }
}
\`\`\`

## Exemples de Mapping

| Demande | chartType | groupBy | metrics | filters |
|---------|-----------|---------|---------|---------|
| "Top 10 par volume" | bar | null | [sum(volumeL)/k] | [] | limit:10 |
| "RÃ©partition par ville" | bar/pie | city | [count, sum(volumeL)/k] | [] |
| "Compare KOLs vs autres" | bar | isKOL | [sum(volumeL)/k, count] | [] |
| "KOLs par spÃ©cialitÃ©" | pie | specialty | [count] | [isKOL=true] |
| "Distribution par risque" | pie | riskLevel | [count, sum(volumeL)/k] | [] |
| "FidÃ©litÃ© vs volume top 15" | composed | null | [sum(volumeL)/k, avg(loyaltyScore)] | [] | limit:15 |
| "Segments par vingtile" | bar | vingtileBucket | [count, sum(volumeL)/k] | [] |

RÃ©ponds UNIQUEMENT avec le JSON, sans aucun texte avant ou aprÃ¨s.`;

const CHART_MODIFY_PROMPT = `Tu es un expert en modification de visualisations de donnÃ©es CRM.

## Graphique Actuel
{CURRENT_CHART}

## Modification DemandÃ©e
{MODIFICATION}

## Instructions
Modifie la spÃ©cification du graphique actuel selon la demande. Conserve les donnÃ©es et filtres existants sauf si la modification les affecte directement.

RÃ¨gles :
- "En camembert/pie" â†’ change chartType en "pie"
- "En barres/bar" â†’ change chartType en "bar"
- "En ligne/courbe" â†’ change chartType en "line"
- "Top X" â†’ change limit Ã  X
- "Ajoute la fidÃ©litÃ©/le volume" â†’ ajoute une mÃ©trique
- "Par ville/spÃ©cialitÃ©/..." â†’ change le groupBy
- "Seulement les KOLs" â†’ ajoute filtre isKOL=true
- "Seulement les pneumologues" â†’ ajoute filtre specialty="Pneumologue"

${DATA_SCHEMA}

RÃ©ponds UNIQUEMENT avec le JSON complet de la nouvelle spÃ©cification (mÃªme format que l'original).`;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// LLM API CLIENT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function getApiKey(): string | null {
  const apiKey = import.meta.env.VITE_GROQ_API_KEY;
  if (!apiKey || apiKey === 'your_groq_api_key_here' || apiKey.length < 10) {
    return null;
  }
  return apiKey;
}

async function callLLM(
  messages: LLMMessage[],
  options: LLMCallOptions = {},
  retries = 1
): Promise<string | null> {
  const apiKey = getApiKey();
  if (!apiKey) return null;

  const {
    temperature = 0.3,
    maxTokens = 4096,
    jsonMode = false,
  } = options;

  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      const body: Record<string, unknown> = {
        model: MODEL,
        messages,
        temperature,
        max_tokens: maxTokens,
        stream: false,
      };

      if (jsonMode) {
        body.response_format = { type: 'json_object' };
      }

      const response = await fetch(GROQ_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`,
        },
        body: JSON.stringify(body),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = (errorData as { error?: { message?: string } }).error?.message ||
          `Groq API error: ${response.status}`;
        // Rate limit or server error â€” worth retrying
        if (response.status === 429 || response.status >= 500) {
          console.warn(`[AICoachEngine] LLM call attempt ${attempt + 1} failed (${response.status}), ${attempt < retries ? 'retrying...' : 'giving up'}`);
          if (attempt < retries) {
            await new Promise(r => setTimeout(r, 1000 * (attempt + 1)));
            continue;
          }
        }
        throw new Error(errorMsg);
      }

      const data = await response.json();
      return data.choices?.[0]?.message?.content || null;
    } catch (err) {
      if (attempt < retries) {
        console.warn(`[AICoachEngine] LLM call attempt ${attempt + 1} error, retrying...`, err);
        await new Promise(r => setTimeout(r, 1000 * (attempt + 1)));
        continue;
      }
      console.error('[AICoachEngine] LLM call failed after retries:', err);
      return null;
    }
  }
  return null;
}

export async function streamLLM(
  messages: LLMMessage[],
  onChunk: (chunk: string) => void,
  options: LLMCallOptions = {}
): Promise<void> {
  const apiKey = getApiKey();
  if (!apiKey) throw new Error('API key not configured');

  const { temperature = 0.3, maxTokens = 4096 } = options;

  const response = await fetch(GROQ_API_URL, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: MODEL,
      messages,
      temperature,
      max_tokens: maxTokens,
      stream: true,
    }),
  });

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}));
    throw new Error(
      (errorData as { error?: { message?: string } }).error?.message ||
      `Groq API error: ${response.status}`
    );
  }

  const reader = response.body?.getReader();
  if (!reader) throw new Error('No reader available');

  const decoder = new TextDecoder();
  let buffer = '';

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split('\n');
    buffer = lines.pop() || '';

    for (const line of lines) {
      const trimmed = line.trim();
      if (trimmed.startsWith('data: ')) {
        const data = trimmed.slice(6);
        if (data === '[DONE]') continue;
        try {
          const parsed = JSON.parse(data);
          const content = parsed.choices?.[0]?.delta?.content;
          if (content) onChunk(content);
        } catch {
          // Ignore incomplete chunks
        }
      }
    }
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PHASE 1 : LLM ROUTER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function routeQuestion(
  question: string,
  chartHistory: ChartHistory[],
  lastAssistantMessage?: string
): Promise<RouterResult | null> {
  // Build chart context for the router
  let chartContext = 'Aucun graphique prÃ©cÃ©dent.';
  if (chartHistory.length > 0) {
    const last = chartHistory[0];
    const dataPreview = last.data.slice(0, 5).map(d => {
      const metrics = Object.entries(d)
        .filter(([k]) => k !== 'name' && !k.startsWith('_'))
        .map(([k, v]) => `${k}: ${v}`)
        .join(', ');
      return `  ${d.name}: ${metrics}`;
    }).join('\n');
    chartContext = `Dernier graphique: "${last.question}"
Type: ${last.spec.chartType} | Titre: ${last.spec.title}
DonnÃ©es: \n${dataPreview}`;
  }

  const routerPrompt = ROUTER_SYSTEM_PROMPT.replace('{CHART_CONTEXT}', chartContext);

  let userContext = question;
  if (lastAssistantMessage) {
    userContext = `[Dernier message assistant: "${lastAssistantMessage.substring(0, 200)}..."]\n\nQuestion: ${question}`;
  }

  const result = await callLLM(
    [
      { role: 'system', content: routerPrompt },
      { role: 'user', content: userContext },
    ],
    { temperature: 0.0, maxTokens: 800, jsonMode: true }
  );

  if (!result) return null;

  try {
    const parsed = JSON.parse(result);
    // Validate and normalize
    const validIntents = ['chart_create', 'chart_modify', 'data_query', 'practitioner_info', 'strategic_advice', 'follow_up', 'general'];
    if (!validIntents.includes(parsed.intent)) {
      parsed.intent = 'general';
    }
    return parsed as RouterResult;
  } catch (err) {
    console.error('[AICoachEngine] Router parse error:', err);
    return null;
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SMART CONTEXT BUILDER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function buildTargetedContext(
  routing: RouterResult,
  question: string,
  periodLabel: string,
  practitioners: Practitioner[],
  upcomingVisits: UpcomingVisit[]
): string {
  const stats = DataService.getGlobalStats();
  const periodMetrics = calculatePeriodMetrics(practitioners, upcomingVisits, 'month');

  // Base context always included: territory overview
  let context = `## Territoire (${periodLabel})
- ${stats.totalPractitioners} praticiens (${stats.pneumologues} pneumo, ${stats.generalistes} MG)
- ${stats.totalKOLs} KOLs | Volume total: ${(stats.totalVolume / 1000).toFixed(0)}K L/an | FidÃ©litÃ© moy: ${stats.averageLoyalty.toFixed(1)}/10
- Visites ${periodLabel}: ${periodMetrics.visitsCount}/${periodMetrics.visitsObjective} (${((periodMetrics.visitsCount / periodMetrics.visitsObjective) * 100).toFixed(0)}%)
- Croissance volume: +${periodMetrics.volumeGrowth.toFixed(1)}% | Nouveaux prescripteurs: ${periodMetrics.newPrescribers}\n`;

  const allPractitioners = DataService.getAllPractitioners();

  switch (routing.dataScope) {
    case 'specific': {
      // Fetch full profiles for specific practitioners
      if (routing.searchTerms.names.length > 0) {
        const matches = allPractitioners.filter(p => {
          const fullName = `${p.firstName} ${p.lastName}`.toLowerCase();
          return routing.searchTerms.names.some(name =>
            fullName.includes(name.toLowerCase()) ||
            p.firstName.toLowerCase().includes(name.toLowerCase()) ||
            p.lastName.toLowerCase().includes(name.toLowerCase())
          );
        });

        if (matches.length > 0) {
          context += `\n## Praticiens TrouvÃ©s (${matches.length})\n`;
          for (const p of matches.slice(0, 10)) {
            context += DataService.getCompletePractitionerContext(p.id);
          }
        } else {
          // Fuzzy search fallback
          for (const name of routing.searchTerms.names) {
            const fuzzy = DataService.fuzzySearchPractitioner(name);
            if (fuzzy.length > 0) {
              context += `\n## RÃ©sultats pour "${name}" (${fuzzy.length})\n`;
              for (const p of fuzzy.slice(0, 5)) {
                context += DataService.getCompletePractitionerContext(p.id);
              }
            }
          }
        }
      }
      break;
    }

    case 'filtered': {
      // Use universal search for filtered results
      const searchResult = universalSearch(question);
      if (searchResult.results.length > 0) {
        context += searchResult.context;
      } else {
        // Fallback: build filtered list manually
        let filtered = allPractitioners;
        if (routing.searchTerms.cities.length > 0) {
          filtered = filtered.filter(p =>
            routing.searchTerms.cities.some(c => p.address.city.toLowerCase().includes(c.toLowerCase()))
          );
        }
        if (routing.searchTerms.specialties.length > 0) {
          filtered = filtered.filter(p =>
            routing.searchTerms.specialties.some(s => p.specialty.toLowerCase().includes(s.toLowerCase()))
          );
        }
        if (routing.searchTerms.isKOL !== null) {
          filtered = filtered.filter(p => p.metrics.isKOL === routing.searchTerms.isKOL);
        }

        context += `\n## Praticiens FiltrÃ©s (${filtered.length})\n`;
        for (const p of filtered.slice(0, 20)) {
          const pubCount = p.news?.filter(n => n.type === 'publication').length || 0;
          context += `- ${p.title} ${p.firstName} ${p.lastName} | ${p.specialty} | ${p.address.city} | V:${(p.metrics.volumeL / 1000).toFixed(0)}K L/an | F:${p.metrics.loyaltyScore}/10 | V${p.metrics.vingtile}${p.metrics.isKOL ? ' | KOL' : ''}${pubCount > 0 ? ` | ${pubCount} pub` : ''}\n`;
        }
        if (filtered.length > 20) {
          context += `... et ${filtered.length - 20} autres\n`;
        }

        // Aggregated stats for the filtered set
        const totalVol = filtered.reduce((s, p) => s + p.metrics.volumeL, 0);
        const kolCount = filtered.filter(p => p.metrics.isKOL).length;
        const avgLoy = filtered.reduce((s, p) => s + p.metrics.loyaltyScore, 0) / (filtered.length || 1);
        context += `\nStats filtrÃ©es: Volume total ${(totalVol / 1000).toFixed(0)}K L/an | ${kolCount} KOLs | FidÃ©litÃ© moy ${avgLoy.toFixed(1)}/10\n`;
      }
      break;
    }

    case 'aggregated': {
      // Send aggregated stats + key lists
      const kols = DataService.getKOLs();
      const atRisk = DataService.getAtRiskPractitioners();
      const topPractitioners = getTopPractitioners(practitioners, 'year', 10);

      context += `\n## Top 10 Prescripteurs (volume annuel)\n`;
      topPractitioners.forEach((p, i) => {
        context += `${i + 1}. ${p.title} ${p.firstName} ${p.lastName} â€” ${p.specialty}, ${p.city} | ${(p.volumeL / 1000).toFixed(0)}K L/an | F:${p.loyaltyScore}/10 | V${p.vingtile}${p.isKOL ? ' | KOL' : ''}\n`;
      });

      context += `\n## KOLs (${kols.length})\n`;
      kols.slice(0, 10).forEach(p => {
        context += `- ${p.title} ${p.firstName} ${p.lastName} (${p.specialty}, ${p.address.city}) â€” ${(p.metrics.volumeL / 1000).toFixed(0)}K L/an | F:${p.metrics.loyaltyScore}/10\n`;
      });

      if (atRisk.length > 0) {
        context += `\n## Praticiens Ã  Risque (${atRisk.length})\n`;
        atRisk.slice(0, 8).forEach(p => {
          context += `- ${p.title} ${p.firstName} ${p.lastName} (${p.address.city}) â€” F:${p.metrics.loyaltyScore}/10 | ${(p.metrics.volumeL / 1000).toFixed(0)}K L/an | Risque: ${p.metrics.churnRisk}${p.metrics.isKOL ? ' | KOL!' : ''}\n`;
        });
      }

      // By city distribution
      const byCity: Record<string, number> = {};
      allPractitioners.forEach(p => { byCity[p.address.city] = (byCity[p.address.city] || 0) + 1; });
      context += `\n## RÃ©partition par Ville\n`;
      Object.entries(byCity).sort((a, b) => b[1] - a[1]).forEach(([city, count]) => {
        context += `- ${city}: ${count}\n`;
      });
      break;
    }

    case 'full':
    default: {
      // Full database context â€” used for open-ended or complex questions
      const searchResult = universalSearch(question);
      if (searchResult.results.length > 0) {
        context += searchResult.context;
      }

      // Include complete practitioner listing
      context += `\n## Base ComplÃ¨te (${allPractitioners.length} praticiens)\n`;
      allPractitioners.forEach(p => {
        const pubCount = p.news?.filter(n => n.type === 'publication').length || 0;
        context += `- ${p.title} ${p.firstName} ${p.lastName} | ${p.specialty} | ${p.address.city} | V:${(p.metrics.volumeL / 1000).toFixed(0)}K | F:${p.metrics.loyaltyScore}/10 | V${p.metrics.vingtile}${p.metrics.isKOL ? ' | KOL' : ''}${pubCount > 0 ? ` | ${pubCount} pub` : ''}\n`;
      });
      break;
    }
  }

  return context;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PHASE 2A : CHART GENERATION / MODIFICATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function generateChart(
  question: string,
  routing: RouterResult,
  chartHistory: ChartHistory[]
): Promise<AICoachResult['chart'] | null> {
  const dataContext = buildChartDataContext();

  let messages: LLMMessage[];

  if (routing.intent === 'chart_modify' && chartHistory.length > 0) {
    // Chart modification: pass current spec + modification request
    const currentChart = chartHistory[0];
    const currentSpec = JSON.stringify(currentChart.spec, null, 2);
    const modPrompt = CHART_MODIFY_PROMPT
      .replace('{CURRENT_CHART}', currentSpec)
      .replace('{MODIFICATION}', routing.chartModification || question);

    messages = [
      { role: 'system', content: modPrompt },
      { role: 'user', content: `Question originale: "${currentChart.question}"\nModification demandÃ©e: "${question}"\n\n${dataContext}` },
    ];
  } else {
    // New chart creation
    let paramHints = '';
    if (routing.chartParams.limit) {
      paramHints += `\nATTENTION: L'utilisateur demande EXACTEMENT ${routing.chartParams.limit} Ã©lÃ©ments.`;
    }
    if (routing.chartParams.chartType) {
      paramHints += `\nATTENTION: L'utilisateur veut un graphique de type "${routing.chartParams.chartType}".`;
    }
    if (routing.chartParams.groupBy) {
      paramHints += `\nATTENTION: Grouper par "${routing.chartParams.groupBy}".`;
    }
    if (routing.searchTerms.isKOL === true) {
      paramHints += `\nATTENTION: Filtrer uniquement les KOLs.`;
    }

    messages = [
      { role: 'system', content: CHART_SYSTEM_PROMPT },
      { role: 'user', content: `${dataContext}\n\nDEMANDE: "${question}"${paramHints}\n\nGÃ©nÃ¨re la spÃ©cification JSON du graphique.` },
    ];
  }

  const chartResponse = await callLLM(messages, {
    temperature: 0.0,
    maxTokens: 1500,
  });

  if (!chartResponse) return null;

  let spec = parseLLMChartResponse(chartResponse);
  if (!spec) return null;

  // Force limit from router if LLM didn't respect it
  if (routing.chartParams.limit && spec.query.limit !== routing.chartParams.limit) {
    spec.query.limit = routing.chartParams.limit;
  }

  // Force chart type from router if specified
  if (routing.chartParams.chartType && spec.chartType !== routing.chartParams.chartType) {
    spec.chartType = routing.chartParams.chartType;
  }

  const chartResult = generateChartFromSpec(spec);

  // Save to history
  addToChartHistory({
    question,
    spec: chartResult.spec,
    data: chartResult.data,
    insights: chartResult.insights,
    timestamp: new Date(),
  });

  return {
    spec: chartResult.spec,
    data: chartResult.data,
    insights: chartResult.insights,
    suggestions: chartResult.suggestions,
    generatedByLLM: true,
  };
}

function buildChartDataContext(): string {
  const stats = DataService.getGlobalStats();
  const allPractitioners = DataService.getAllPractitioners();
  const cities = [...new Set(allPractitioners.map(p => p.address.city))];

  const kolsBySpecialty = allPractitioners
    .filter(p => p.metrics.isKOL)
    .reduce((acc, p) => {
      acc[p.specialty] = (acc[p.specialty] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);

  return `DONNÃ‰ES ACTUELLES :
- ${stats.totalPractitioners} praticiens (${stats.pneumologues} Pneumologues, ${stats.generalistes} MG)
- ${stats.totalKOLs} KOLs (Pneumo: ${kolsBySpecialty['Pneumologue'] || 0}, MG: ${kolsBySpecialty['MÃ©decin gÃ©nÃ©raliste'] || 0})
- Volume total: ${Math.round(stats.totalVolume / 1000)}K L/an
- FidÃ©litÃ© moyenne: ${stats.averageLoyalty.toFixed(1)}/10
- Villes: ${cities.join(', ')}`;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PHASE 2B : TEXT RESPONSE GENERATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function generateTextResponse(
  question: string,
  routing: RouterResult,
  dataContext: string,
  conversationHistory: ConversationMessage[],
  chartResult: AICoachResult['chart'] | null,
  periodLabel: string
): Promise<string | null> {
  const messages: LLMMessage[] = [
    { role: 'system', content: COACH_SYSTEM_PROMPT },
  ];

  // Add data context as a system message (clear separation from conversation)
  messages.push({
    role: 'system',
    content: `## DonnÃ©es Disponibles (${periodLabel})\n${dataContext}`,
  });

  // Add chart context if a chart was just generated
  if (chartResult) {
    const chartSummary = chartResult.data.slice(0, 8).map(d => {
      const metrics = Object.entries(d)
        .filter(([k]) => k !== 'name' && !k.startsWith('_'))
        .map(([k, v]) => `${k}: ${v}`)
        .join(', ');
      return `  ${d.name}: ${metrics}`;
    }).join('\n');

    messages.push({
      role: 'system',
      content: `## Graphique GÃ©nÃ©rÃ©
Titre: ${chartResult.spec.title}
Type: ${chartResult.spec.chartType}
DonnÃ©es:\n${chartSummary}
Insights: ${chartResult.insights.join(' | ')}

INSTRUCTIONS: Un graphique a Ã©tÃ© gÃ©nÃ©rÃ© et sera affichÃ©. Ta rÃ©ponse textuelle doit COMPLÃ‰TER le graphique avec une analyse, pas le dÃ©crire entiÃ¨rement. Sois synthÃ©tique â€” le graphique parle de lui-mÃªme.`,
    });
  }

  // Add conversation history (last 10 turns max)
  const recentHistory = conversationHistory.slice(-10);
  for (const msg of recentHistory) {
    messages.push({
      role: msg.role,
      content: msg.content,
    });
  }

  // Add current question
  messages.push({
    role: 'user',
    content: question,
  });

  // Adjust temperature based on intent
  let temperature = 0.3;
  if (routing.intent === 'strategic_advice') temperature = 0.5;
  if (routing.intent === 'general') temperature = 0.6;

  return callLLM(messages, { temperature, maxTokens: 4096 });
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DIRECT LLM RESPONSE (Resilient fallback â€” bypasses routing)
// Used when the router fails but the LLM API is still reachable
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function buildGeneralContext(
  periodLabel: string,
  practitioners: Practitioner[],
  upcomingVisits: UpcomingVisit[],
  question: string
): string {
  const stats = DataService.getGlobalStats();
  const periodMetrics = calculatePeriodMetrics(practitioners, upcomingVisits, 'month');
  const allPractitioners = DataService.getAllPractitioners();
  const kols = DataService.getKOLs();
  const atRisk = DataService.getAtRiskPractitioners();
  const topPractitioners = getTopPractitioners(practitioners, 'year', 10);

  // Try universal search for relevant context
  const searchResult = universalSearch(question);
  const searchContext = searchResult.results.length > 0 ? searchResult.context : '';

  // By city distribution
  const byCity: Record<string, number> = {};
  allPractitioners.forEach(p => { byCity[p.address.city] = (byCity[p.address.city] || 0) + 1; });

  let context = `## Territoire (${periodLabel})
- ${stats.totalPractitioners} praticiens (${stats.pneumologues} pneumo, ${stats.generalistes} MG)
- ${stats.totalKOLs} KOLs | Volume total: ${(stats.totalVolume / 1000).toFixed(0)}K L/an | FidÃ©litÃ© moy: ${stats.averageLoyalty.toFixed(1)}/10
- Visites ${periodLabel}: ${periodMetrics.visitsCount}/${periodMetrics.visitsObjective} (${((periodMetrics.visitsCount / periodMetrics.visitsObjective) * 100).toFixed(0)}%)
- Croissance volume: +${periodMetrics.volumeGrowth.toFixed(1)}% | Nouveaux prescripteurs: ${periodMetrics.newPrescribers}

## Top 10 Prescripteurs
${topPractitioners.map((p, i) => `${i + 1}. ${p.title} ${p.firstName} ${p.lastName} â€” ${p.specialty}, ${p.city} | ${(p.volumeL / 1000).toFixed(0)}K L/an | F:${p.loyaltyScore}/10 | V${p.vingtile}${p.isKOL ? ' | KOL' : ''}`).join('\n')}

## KOLs (${kols.length})
${kols.slice(0, 10).map(p => `- ${p.title} ${p.firstName} ${p.lastName} (${p.specialty}, ${p.address.city}) â€” ${(p.metrics.volumeL / 1000).toFixed(0)}K L/an | F:${p.metrics.loyaltyScore}/10`).join('\n')}

## Praticiens Ã  Risque (${atRisk.length})
${atRisk.slice(0, 8).map(p => `- ${p.title} ${p.firstName} ${p.lastName} (${p.address.city}) â€” F:${p.metrics.loyaltyScore}/10 | ${(p.metrics.volumeL / 1000).toFixed(0)}K L/an | Risque: ${p.metrics.churnRisk}${p.metrics.isKOL ? ' | KOL!' : ''}`).join('\n')}

## RÃ©partition par Ville
${Object.entries(byCity).sort((a, b) => b[1] - a[1]).map(([city, count]) => `- ${city}: ${count}`).join('\n')}
${searchContext}
## Base ComplÃ¨te (${allPractitioners.length} praticiens)
${allPractitioners.map(p => {
  const pubCount = p.news?.filter(n => n.type === 'publication').length || 0;
  return `- ${p.title} ${p.firstName} ${p.lastName} | ${p.specialty} | ${p.address.city} | V:${(p.metrics.volumeL / 1000).toFixed(0)}K | F:${p.metrics.loyaltyScore}/10 | V${p.metrics.vingtile}${p.metrics.isKOL ? ' | KOL' : ''}${pubCount > 0 ? ` | ${pubCount} pub` : ''}`;
}).join('\n')}`;

  return context;
}

async function generateDirectResponse(
  question: string,
  conversationHistory: ConversationMessage[],
  periodLabel: string,
  practitioners: Practitioner[],
  upcomingVisits: UpcomingVisit[]
): Promise<string | null> {
  const context = buildGeneralContext(periodLabel, practitioners, upcomingVisits, question);

  const messages: LLMMessage[] = [
    { role: 'system', content: COACH_SYSTEM_PROMPT },
    { role: 'system', content: `## DonnÃ©es Disponibles (${periodLabel})\n${context}` },
  ];

  // Add conversation history (excluding current question â€” it will be added separately)
  const recentHistory = conversationHistory.slice(-10);
  for (const msg of recentHistory) {
    messages.push({ role: msg.role, content: msg.content });
  }

  messages.push({ role: 'user', content: question });

  return callLLM(messages, { temperature: 0.4, maxTokens: 4096 }, 1);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// LOCAL FALLBACK
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function generateLocalResponse(
  question: string,
  practitioners: Practitioner[],
  userObjectives: { visitsMonthly: number; visitsCompleted: number },
  wantsChart: boolean
): AICoachResult {
  if (wantsChart) {
    const chartResult = generateChartLocally(question);
    if (chartResult && chartResult.data.length > 0) {
      const firstMetric = chartResult.spec.query.metrics[0]?.name || 'value';
      const topItems = chartResult.data.slice(0, 3);

      addToChartHistory({
        question,
        spec: chartResult.spec,
        data: chartResult.data,
        insights: chartResult.insights,
        timestamp: new Date(),
      });

      return {
        textContent: `**${chartResult.spec.title}**\n\n${chartResult.spec.description}\n\n**RÃ©sumÃ© :**\n${chartResult.insights.map(i => `â€¢ ${i}`).join('\n')}\n\n**Top ${Math.min(3, topItems.length)} :**\n${topItems.map((item, i) => `${i + 1}. **${item.name}** : ${item[firstMetric]}`).join('\n')}`,
        chart: {
          spec: chartResult.spec,
          data: chartResult.data,
          insights: chartResult.insights,
          suggestions: chartResult.suggestions,
          generatedByLLM: false,
        },
        suggestions: chartResult.suggestions,
        source: 'local',
      };
    }
  }

  // Text fallback
  const response = generateCoachResponse(question, practitioners, userObjectives);
  return {
    textContent: response.message,
    practitioners: response.practitioners,
    suggestions: response.insights?.slice(0, 3),
    source: 'local',
  };
}

// Simple local check for chart-like questions (used only for fallback routing)
function looksLikeChartRequest(question: string): boolean {
  const q = question.toLowerCase().normalize('NFD').replace(/[\u0300-\u036f]/g, '');
  return /graphique|graph|chart|diagramme|visualis|courbe|barres?|camembert|histogramme|montre[- ]?moi|affiche|repartition|distribution|top\s*\d+|classement|compare|par ville|par specialite|par segment|par vingtile|par risque/.test(q);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MAIN PIPELINE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export async function processQuestion(
  question: string,
  conversationHistory: ConversationMessage[],
  periodLabel: string,
  practitioners: Practitioner[],
  upcomingVisits: UpcomingVisit[],
  userObjectives: { visitsMonthly: number; visitsCompleted: number }
): Promise<AICoachResult> {
  const chartHistory = getChartHistory();
  const lastAssistant = conversationHistory.filter(m => m.role === 'assistant').slice(-1)[0]?.content;

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // PIPELINE RÃ‰SILIENT : Router â†’ Targeted LLM â†’ Direct LLM â†’ Local
  //
  // Si Phase 1 (routeur) Ã©choue â†’ on essaie quand mÃªme le LLM direct
  // Si Phase 2 (rÃ©ponse) Ã©choue â†’ on essaie le LLM direct sans routing
  // Si tout Ã©choue â†’ fallback local
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  // â”€â”€â”€ Phase 1: LLM Routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const routing = await routeQuestion(question, chartHistory, lastAssistant);

  if (routing) {
    console.log('[AICoachEngine] Router:', routing.intent, routing.dataScope, routing.needsChart ? 'ğŸ“Š' : 'ğŸ’¬');

    // â”€â”€â”€ Build Targeted Context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const dataContext = buildTargetedContext(routing, question, periodLabel, practitioners, upcomingVisits);

    // â”€â”€â”€ Phase 2A: Chart Generation (if needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let chartResult: AICoachResult['chart'] | null = null;
    if (routing.needsChart) {
      chartResult = await generateChart(question, routing, chartHistory);
      if (!chartResult) {
        console.log('[AICoachEngine] Chart LLM failed, trying local chart');
        const localChart = generateChartLocally(question);
        if (localChart && localChart.data.length > 0) {
          addToChartHistory({
            question,
            spec: localChart.spec,
            data: localChart.data,
            insights: localChart.insights,
            timestamp: new Date(),
          });
          chartResult = {
            spec: localChart.spec,
            data: localChart.data,
            insights: localChart.insights,
            suggestions: localChart.suggestions,
            generatedByLLM: false,
          };
        }
      }
    }

    // â”€â”€â”€ Phase 2B: Text Response Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const textResponse = await generateTextResponse(
      question,
      routing,
      dataContext,
      conversationHistory,
      chartResult,
      periodLabel
    );

    if (textResponse) {
      // â”€â”€â”€ SUCCESS: Full pipeline worked â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      const result: AICoachResult = {
        textContent: textResponse,
        source: 'llm',
      };

      if (chartResult) {
        result.chart = chartResult;
        result.suggestions = chartResult.suggestions;
      }

      // For practitioner_info intent, extract matching practitioners for card display
      if (routing.intent === 'practitioner_info' && routing.searchTerms.names.length > 0) {
        result.practitioners = findPractitionerCards(routing.searchTerms.names);
      }

      return result;
    }

    // Text response failed â€” fall through to direct LLM
    console.log('[AICoachEngine] Text LLM failed after routing, trying direct LLM...');
  } else {
    console.log('[AICoachEngine] Router failed, trying direct LLM...');
  }

  // â”€â”€â”€ FALLBACK 1: Direct LLM (no routing) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // The router or text response failed, but the API might still work.
  // Try a direct call with general context.
  const directResponse = await generateDirectResponse(
    question,
    conversationHistory,
    periodLabel,
    practitioners,
    upcomingVisits
  );

  if (directResponse) {
    console.log('[AICoachEngine] Direct LLM succeeded');
    return {
      textContent: directResponse,
      source: 'llm',
    };
  }

  // â”€â”€â”€ FALLBACK 2: Local response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  console.log('[AICoachEngine] All LLM calls failed, using local fallback');
  return generateLocalResponse(
    question,
    practitioners,
    userObjectives,
    looksLikeChartRequest(question)
  );
}

// Helper: find practitioner cards for display
function findPractitionerCards(names: string[]): (Practitioner & { daysSinceVisit?: number })[] {
  const allPractitioners = DataService.getAllPractitioners();
  const matches = allPractitioners.filter(p => {
    const fullName = `${p.firstName} ${p.lastName}`.toLowerCase();
    return names.some(name =>
      fullName.includes(name.toLowerCase()) ||
      p.firstName.toLowerCase().includes(name.toLowerCase()) ||
      p.lastName.toLowerCase().includes(name.toLowerCase())
    );
  });

  if (matches.length === 0) return [];

  const today = new Date();
  return matches.slice(0, 5).map(p => {
    const adapted = adaptPractitionerProfile(p);
    const lastVisit = p.lastVisitDate ? new Date(p.lastVisitDate) : null;
    const daysSinceVisit = lastVisit
      ? Math.floor((today.getTime() - lastVisit.getTime()) / (1000 * 60 * 60 * 24))
      : 999;
    return { ...adapted, daysSinceVisit };
  });
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// UTILITIES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export function isLLMConfigured(): boolean {
  return getApiKey() !== null;
}
